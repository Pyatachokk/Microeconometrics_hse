# Потанин Богдан Станиславович
# Микроэконометрика
# Семинар №1
# Метод максимального правдоподобия и численные методы оптимизации
#
#--------------
# РАЗДЕЛ №0. Численный метод расчета градиента функции
#--------------
#
# Для начала кратко ознакомтесь с тем, что представляет из себя численный метод дифференцирования
# https://en.wikipedia.org/wiki/Numerical_differentiation
#
# Запрограммируем функцию x1^2+x2^3-x1*x2
my_func <- function(x)
{
  return(x[1] ^ 2 + x[2] ^ 3 - x[1] * x[2])
}
# Найдем её частные производные в точке (2,3) при помощи численного метода дифференцирования
# При этом настоящие значения производных в этой точке равны 1 и 25 соответственно
delta <- 0.00001                                                        # выберем маленькое приращение
d_my_func_d_x1 <- (my_func(c(2 + delta, 3)) - my_func(c(2, 3))) / delta # частная производная по x1
d_my_func_d_x2 <- (my_func(c(2, 3 + delta)) - my_func(c(2, 3))) / delta # частная производная по x2
matrix(c(d_my_func_d_x1, d_my_func_d_x2), ncol = 1)                     # численный градиент
# Теперь повторим это при помощи специальной встроенной функции
install.packages("numDeriv")
library("numDeriv")
grad(func = my_func,         # выбираем фукцию которую будем дифференцировать
     x = c(2, 3))            # выбираем точку, в которой будем искать градиент (вектор частных производных)
# По аналогии нетрудно рассчитать и Гессиан
hessian(func = my_func, x = c(2, 3))
  # ЗАДАНИЯ
  # 1. Найдите градиент функции exp(x1^x2/log(sqrt(x3)))) в точке (1,2,3) численным методом
  # 2. Найдите градиент функции (x1*sin(x2*cos(x3)))^x1 в точке (3,2,1) численным методом
delta <- 0.0000001   
my_func_2 = function(x){
  exp(x[1]^(x[2])/log(sqrt(x[3])))
}

d_my_func_2_d_x1 <- (my_func_2(c(1+delta, 2, 3)) - my_func_2(c(1,2,3))) / delta 
d_my_func_2_d_x2 <- (my_func_2(c(1, 2+delta, 3)) - my_func_2(c(1,2,3))) / delta 
d_my_func_2_d_x3 <- (my_func_2(c(1, 2, 3+delta)) - my_func_2(c(1,2,3))) / delta 

matrix(c(d_my_func_2_d_x1, d_my_func_2_d_x2, d_my_func_2_d_x3))

grad(func = my_func_2,         # выбираем фукцию которую будем дифференцировать
     x = c(1, 2, 3))  

my_func_2 = function(x){
  (x[1]*sin(x[2]*cos(x[3])))^x[1]
}

d_my_func_2_d_x1 <- (my_func_2(c(3+delta, 2, 1)) - my_func_2(c(3,2,1))) / delta 
d_my_func_2_d_x2 <- (my_func_2(c(3, 2+delta, 1)) - my_func_2(c(3,2,1))) / delta 
d_my_func_2_d_x3 <- (my_func_2(c(3,2,1+delta)) - my_func_2(c(3,2,1))) / delta 

matrix(c(d_my_func_2_d_x1, d_my_func_2_d_x2, d_my_func_2_d_x3))

grad(func = my_func_2,         # выбираем фукцию которую будем дифференцировать
     x = c(3,2,1))  
#
#--------------
# РАЗДЕЛ №1. Решение оптимизационных задач при помощи численных методов
#--------------
#
# Для ознакомления с численными методами оптимизации рекомендуется прочесть 
# https://www.researchgate.net/publication/313867455_Numerical_Optimization_Methods_in_Economics
#
# Для начала запрограммируем крайне упрощенный вариант градиентного спуска (gradient descent)
# с фиксированным learning rate и условием остановки, определяемым лишь числом итераций
# https://en.wikipedia.org/wiki/Gradient_descent
gd <- function(fn,                               # оптимизируемая функция
               x0,                               # вектор начальных значений
               iter,                             # число итераций
               learning_rate = 0.01,             # название говорит само за себя
               is_max = TRUE)                    # если TRUE, то функция максимизируется, а если FALSE - минимизируется
{
  
  if (is_max)                                    # по умолчанию метод градиентного спуска предназначен для
  {                                              # поиска локального минимума, поэтому, для поиска максимума
    learning_rate <- learning_rate * (-1)        # необходимо изменить знак learning rate на противоположный
  }

  for (i in 1:iter)
  {
    x0 <- (x0 - learning_rate * grad(fn, x0))    # повторим процедуру iter раз 
  }
  
  return(x0)
}
# Допустим, что нам необходимо найти локальный максимум некоторой функции.
# Рассмотрим несколько примеров того, как это можно сделать применяя численные методы оптимзации.
# Пример №1: 2ln(x)+3ln(5-x)
# Запрограммируем функцию
func_1 <- function(x)
{
  value <- 2 * log(x) + 3 * log(5 - x) # считаем значение функции
  
  return(value)                        # возвращаем значение, посчитанное в функции                   
}
#Для начала воспользуемся собственным оптимизатором
gd(fn = func_1, x0 = 3, iter = 10000, learning_rate = 0.01)
# Теперь применим встроенный оптимизатор
x0 <- 3                                # задаем любую начальную точку, в которой наша функция определена
opt_result <- optim(par = x0,          # начальная точка
                    fn = func_1,       # минимизируемая функция
                    method = "BFGS",   # метод численной оптимизации
                    control = list("maxit" = 10000,          # максимальное количество итераций
                                   "fnscale" = -1))          # ставим -1 чтобы сделать задачу максимизационной.
                                                             # дело в том, что по умолчанию optim является минимизатором и
                                                             # для того, чтобы обратить задачу в максимизационную, необходимо
                                                             # домножить функцию на -1
# Рассмотрим полученные по результатам решения максимизационной задачи значения
opt_result$par   # точка максимума
opt_result$value # значение функции в точке максимума
#
# Обратите внимание, что недостатком большинства численных методов оптимизации является то, что они, как
# правило, находят лишь один локальный, а не глобальный максимум. Причем, то, какой локальный максимум
# будет найден, может зависеть от начальной точки x0
#
# Пример №2. (x1+x2+1)^2-5x1^2-10x2^2
# Следует учесть, что все аргументы функции задаются в виде одного вектора x.
# Поэтому обращаться к x1 будем как x[1], а к x2 через x[2].
# Запрограммируем функцию
func_2 <- function(x)
{
  value <- (x[1] + x[2] + 1) ^ 2 - 5 * x[1] ^ 2 - 10 * x[2] ^ 2
  
  return(value)                       
}      
#Для начала воспользуемся собственным оптимизатором
gd(fn = func_2, x0 = c(1, 2), iter = 10000, learning_rate = 0.01)
# Применим оптимизатор
x0 <- c(1,2)
opt_result <- optim(par =  x0,    
                    fn = func_2,
                    control = list("maxit" = 1000,
                                   "fnscale" = -1))
# Рассмотрим полученные по результатам решения максимизационной задачи значения
opt_result$par   # точка максимума
opt_result$value # значение функции в точке максимума
  # ЗАДАНИЯ
  # 1. Найдите максимум функции ln(x) - exp(x).
  # 2. Найдите максимум функции (x-y) ^ (1/2) - x / y ^ (1/3).
  # 3. Эксперементируя с начальной точкой найдите оба локальных максимума (оба же будут глобальными)
  #    функции -(x^2-1)^2-(x^2*y-x-1)^2.
  # 4. Напишите собственный оптимизатор на основе градиентного спуска (gradient descent) c
  #    изменяющимся learning rate и грамотно прописанными termination condition.
  #    https://en.wikipedia.org/wiki/Gradient_descent
  #    В качестве входных аргументов оптимизатор должен принимать функцию и вектор начальных точек.
  #    Возвращать оптимизатор должен вектор точек, в которых функция достигает локального максимума.
  # 5. Повторите все задания и примеры с помощью собственного оптимизатора.
#
#--------------
# РАЗДЕЛ №2. Максимизация функции правдоподобия при помощи численных методов
#--------------
#
# Запрограммируем функцию для расчета логарифма функции правдоподобия
# нормального распределения при заданной реализации выборки
lnL <- function (x, my_sample)                                 # my_sample - реализация выборки как аргумент функции
{
  mu <- x[1]                                                   # для удобства создадим отдельные переменные под
  sigma2 <- x[2]                                               # значения оптимизируемых параметров
  
  L_vector <- dnorm(my_sample,                                 # считаем функцию плотности для реализации каждого элемента выборки
                    mean = mu,                                 # задаем математическое ожидание
                    sd = sqrt(sigma2))                         # задаем стандартное отклонение как корень из дисперсии
  
  lnL_value <- sum(log(L_vector))                              # считаем значение логарифма функции правдоподобия
  
  return(lnL_value)                                            # возвращаем значение логарифма функции правдоподобия при
                                                               # заданных значениях параметров
}
# Симулируем выборку из нормального распределения
n <- 10000                                 # объем выборки: чем он больше, тем выше эффективность ММП оценок
mu <- 1                                    # математическое ожидание
sigma2 <- 25                               # дисперсия
my_sample <- rnorm(n,                      # симулируем выборку из нормального распределения объема n,
                   mu,                     # с математическим ожиданием mu
                   sqrt(sigma2))           # и стандартным отклонением равным, очевидно, квадратному корню из дисперсии
# Для начала найдем оценки mu и sigma^2 используя аналитическую формулу
mu_mle_0 <- mean(my_sample)
sigma2_mle_0 <- mean((my_sample-mu_mle_0)^2)
# Теперь воспользуемся численным методом
x0 <- c(0, 1)                    # в качестве начальной точки возьмем параметры стандартного
                                 # нормального распределения
opt_mle <- optim(par = x0, fn = lnL, 
                 my_sample = my_sample,                   # в конце задаем дополнительный аргумент
                 method = "Nelder-Mead", 
                 hessian = TRUE,                          # возвращаем Гессиан
                 control = list("maxit" = 1000,           # максимальное количество итераций
                                fnscale = -1,             # ставим -1 чтобы сделать задачу максимизационной
                                "reltol" = 1e-32))        # условие остановки алгоритма (termination condition)
mu_mle_1 <- opt_mle$par[1]
sigma2_mle_1 <- opt_mle$par[2]
# Сравним результаты и убедимся, что они практически не различаются
cbind(c(mu_mle_0,mu_mle_1), c(sigma2_mle_0, sigma2_mle_1))
# Найдем аналитическую оценку ковариационной матрицы
cov_mle_0 <- matrix(c(sigma2_mle_0 / n, 0, 0, 2 * (sigma2_mle_0 ^ 2) / n), ncol=2)
# Найдем оценку ковариационной матрицы используя Гессианный метод
fisher_1 <- (-1) * opt_mle$hessian                        # реализация оценки информации фишера
cov_mle_1 <- solve(fisher_1)                              # реализация оценки ковариационной матрицы оценок
# Наконец, найдем истинную ковариационную матрицу
cov_true <- matrix(c(sigma2 / n, 0, 0, 2 * (sigma2 ^ 2) / n), ncol=2)
# Сравним полученные результаты
cbind(diag(cov_mle_0), diag(cov_mle_1), diag(cov_true))
# Протестируем гипотезу H0:mu=1.05
mu_H0 = 1.05
  # Используя аналитические оценки
z_0 <- (mu_mle_0 - mu_H0) / sqrt(cov_mle_0[1,1])          # значение тестовой статистики
pvalue_0 <- 2 * min(pnorm(z_0), 1-pnorm(z_0))             # p-value
  # Используя численные оценки
z_1 <- (mu_mle_1 - mu_H0) / sqrt(cov_mle_1[1,1])
pvalue_1 <- 2 * min(pnorm(z_1), 1-pnorm(z_1))
  # Сравним полученные p-value и убедимся, что их различие невелико
c(pvalue_0, pvalue_1)
# Теперь, используя дельта метод проверим гипотезу H0: sigma*exp(mu)=11
# Для начала найдем распределение этого выражения, обозначив его через v
v_H0 <- 11
v_mle_0 <- sqrt(sigma2_mle_0) * exp(mu_mle_0)
v_mle_1 <- sqrt(sigma2_mle_1) * exp(mu_mle_1)
# Найдем значение градиента рассматриваемого выражения
v_gradient_0 <- matrix(c(sigma2_mle_0 * exp(mu_mle_0), (0.5 / sqrt(sigma2_mle_0)) * exp(mu_mle_0)), ncol = 1)
v_gradient_1 <- matrix(c(sigma2_mle_1 * exp(mu_mle_1), (0.5 / sqrt(sigma2_mle_1)) * exp(mu_mle_1)), ncol = 1)
# Воспользуемся дельта методом для нахождения ковариационной матрицы v которая,
# в данном случае, будучи одномерной, совпадает с дисперсией
v_cov_0 <- t(v_gradient_0) %*% cov_mle_0 %*% v_gradient_0
v_cov_1 <- t(v_gradient_1) %*% cov_mle_1 %*% v_gradient_1
# Осуществим проверку гипотезы
  # Используя аналитические оценки
z_0_new <- (v_mle_0 - v_H0) / sqrt(v_cov_0)
pvalue_0_new <- 2 * min(pnorm(z_0_new), 1-pnorm(z_0_new))
  # Используя численные оценки
z_1_new <- (v_mle_1 - v_H0) / sqrt(v_cov_1)
pvalue_1_new <- 2 * min(pnorm(z_1_new), 1-pnorm(z_1_new))
  # Сравним полученные p-value и убедимся, что их различие незначительно
c(pvalue_0_new, pvalue_1_new)
  # ЗАДАНИЯ
  # 1. Повторите описанные выше шаги для распределения Пуассона с параметром lambda = 5.
  # 2. Повторите шаги, относящиеся к численным методам и Гессианному методу оценивания
  #    ковариационной матрицы оценок в отношении бета распределения (см. функцию rbeta())
  #    с параметрами rshape1 = 2 и rshape2 = 7.
#--------------
# РАЗДЕЛ №3. Приложение к нелинейному по оцениваемым параметрам регрессионному анализу
#--------------
# Установим пакет для работы с многомерным нормальным распределением
install.packages("mvtnorm")
library("mvtnorm")
# Осуществим симуляцию
n <- 10000                                     # количество наблюдений
mu_X <- c(0,0)                                 # вектор математических ожиданий независимых переменных
cov_X <- matrix(c(1, 0.5, 0.5, 1), ncol=2)     # ковариационная матрица независимых переменных
X <- abs(rmvnorm(n, mu_X, cov_X))              # матрица независимых переменых
df = 10                                        # число степеней свободы распределения стьюдента (t-распределение)
epsilon <- rt(n, df)                           # симулируем n случайных ошибок из распределения стьюдента 
                                               # с df степенями свободы
beta_0 <- 0.5                                  # константа
beta_1 <- 0.75                                 #коэффициент при первой независимой переменной
beta_2 <- 0.9                                  #коэффициент при первой независимой переменной
y <- beta_0 + X[, 1] ^ beta_1 + beta_2 ^ X[, 2] + epsilon
# Запрограммируем функцию для расчета логарифма функции правдоподобия 
# в соответствии с обозначенным выше процессом генерации данных
lnL <- function(x, X, y)
{
  # Для удобства запишем значения оцениваемых параметров
  # в отдельные переменные
  df <- x[1]
  beta_0 <- x[2]
  beta_1 <- x[3]
  beta_2 <- x[4]
  
  # Расчитаем вклад каждого наблюдения
  L_vector <- dt(y - beta_0 - X[, 1] ^ beta_1 - beta_2 ^ X[, 2], df)
  
  # Вернем значение
  return(sum(log(L_vector)))
}
# Теперь воспользуемся численным методом
x0 <- c(5, -2, 5, 2)                                      # возьмем далекую (плохую) начальную точку
                                                          # так как на практике хорошую подобрать
                                                          # удается довольно редко
opt_mle <- optim(par = x0, fn = lnL, 
                 X = X,
                 y = y,
                 method = "Nelder-Mead", 
                 hessian = TRUE,                          # возвращаем Гессиан
                 control = list("maxit" = 1000,           # максимальное количество итераций
                                fnscale = -1,             # ставим -1 чтобы сделать задачу максимизационной
                                "reltol" = 1e-32))        # условие остановки алгоритма (termination condition)
df_mle <- opt_mle$par[1]
beta_0_mle <- opt_mle$par[2]
beta_1_mle <- opt_mle$par[3]
beta_2_mle <- opt_mle$par[4]
# Сравним оценки с истинными значениями и убедимся, что они весьма схожи
cbind(c(df_mle, beta_0_mle, beta_1_mle, beta_2_mle), 
      c(df, beta_0, beta_1, beta_2))
# Найдем реализацию оценки ковариационной матрицы
cov_mle <- (-1) * solve(opt_mle$hessian)
rownames(cov_mle) <- c("df", "beta0", "beta1", "beta2")                 # для удобства дадим имена строкам и
colnames(cov_mle) <- rownames(cov_mle)                                  # столбцам оценки ковариационной матрицы
# Протестируем гипотезы
  # H0: beta_1 = 0.7
beta_1_H0 <- 0.7
z <- (beta_1_mle - beta_1_H0) / sqrt(cov_mle[3,3])
pvalue <- 2 * min(pnorm(z), 1-pnorm(z))
  # H0: beta_1^beta_0 + beta_0*beta_2 = 1.35
v_H0 = 1.35
v_mle <- beta_1_mle ^ beta_0_mle + beta_0_mle * beta_2_mle
v_grad <- matrix(
            c(0,                                                        # производная по df
            beta_1_mle ^ beta_0_mle * log(beta_1_mle) + beta_2_mle,     # производная по beta_0
            beta_0_mle * beta_1_mle ^ (beta_0_mle - 1),                 # производная по beta_1
            beta_0_mle), ncol=1)                                        # производная по beta_2         
v_as_var <- t(v_grad) %*% cov_mle %*% v_grad                            # считаем реализацию оценки асимптотичесой дисперсии
z <- (v_mle - v_H0) / sqrt(v_as_var)                                    # рассчитываем статистику теста
pvalue <- 2 * min(pnorm(z), 1-pnorm(z))                                 # вычисляем p-value
# Теперь перейдем к работе с предельными эффектами
# Для начала найдем реализацию оценки предельного эффекта при x1=2
x1_point <- 2
me_mle <- beta_1_mle * x1_point ^ (beta_1_mle - 1)
# Найдем реализацию оценки асимптотической дисперсии оценки предельного эффекта
me_grad <- matrix(c(0,                                  # производная по df
                    0,                                  # производная по beta_0
                    x1_point ^ (beta_1_mle - 1) *       # производная по beta_1
                    (beta_1_mle * log(x1_point) + 1),   
                    0), ncol = 1)                       # производная по beta_2
me_asy_var <- t(me_grad) %*% cov_mle %*% me_grad        # оценка дисперсии оценки предельного эффекта
# Протестируем гипотезу
z <- (me_mle - 0) / sqrt(me_asy_var)                    # статистика теста
pvalue <- 2 * min(pnorm(z), 1-pnorm(z))                 # рассчитываем p-value
  # ЗАДАНИЯ
  # 1. Проверьте, как изменится точность оценок, если в функции правдоподобия
  #    использовать не распределение стьюдента, а логистическое распределение
  #    (функция dlogis()), где параметр df задает значение аргумента scale
  #    в функции dlogis (параметр локации логистического распределения). 
  #    При этом подберите приемлимую начальную точку, например, положив x0 = c(1, 1, 1, 1).
  # 2. Проверьте H0:log(beta_0+beta_2)=sqrt(beta_0*beta_1*beta_2)
  # 3. При помощи LR теста проверьте гипотезу beta_0 = beta_1.
  # 4. Оцените предельный эффект для beta_2 и проверьте гипотезу о его равенстве 1.
  # 5. Самостоятельно придумайте модель, запишите её процесс генерации
  #    и попытайтесь оценить параметры модели численными методами.